{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量和最简单的神经网络\n",
    "\n",
    "## 向量\n",
    "\n",
    "向量(Vector)是深度学习，GenAI领域的基础元素。 \n",
    "\n",
    "其实向量可以简单地理解为数组,一维向量就是只有一个元素的数组，二维向量就是有两个元素的数组，三维向量是有三个元素的数组...\n",
    "\n",
    "学术文章里面严谨一点的数学表达如下，这是一个n维向量，相当于是有n个元素的数组\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{x} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "有时候为了排版方便还会用下面这种形式, 其中x右上角的字母T表示“转置”的意思，就是把向量换个了方向，在现在这个具体的例子里就是把从上到下的顺序换成从左到右:\n",
    "\n",
    "\\begin{align*}\n",
    "x^T = \\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "这里面关于维度这个词的使用很容易导致一些混淆，一般在程序员心里都会对一维数组，二维数组，三维数组有很深的印象，很容易把数组的维度和向量的维度混淆起来。\n",
    "\n",
    "因为维度这个词广泛的适用性，在深度学习的很多地方都会使用到这个词，表达的意思还往往是不同的，所以我们在初步接触这个维度这个词的时候需要仔细思考一下。\n",
    "\n",
    "如果有一个一维数组，数组长度是5，就是里面有5个元素，那么这个数组的维度是1， 这个数组对应的向量的维度是5.\n",
    "\n",
    "再比如，有一个二维数组，形状是 [4 * 6]的，也就是说有4行，6列的数组，那么这个数组自己的维度是 2， 而向量的维度是 4 * 6 = 24 维。\n",
    "\n",
    "其实，一个 [4 * 6]的二维数组也可以当作是一个 24个元素的一维数组来看，里面向量的维度还是 24 维。\n",
    "\n",
    "简单讲，向量的维度由数组内的元素个数决定。\n",
    "\n",
    "用一个不是特别准确的方式，我们可以把数组的维度对应到深度学习的一些概念里，一维数组对应的概念是向量，二维数组对应的概念是矩阵，三维数组对应的概念是张量，就是我们常说的tensor, 更多维度的数组就对应高维张量了，不再给特殊的名词。\n",
    "\n",
    "因为深度学习中经常把一组向量组合起来同时使用，所以后面会经常看到张量，也就是tensor，的使用。\n",
    "\n",
    "为了更形象具体的展示向量，人们经常把向量画在坐标轴上，下面是坐标轴上展示的二维向量和三维向量。因为我们生活在三维空间里，所以我们对三维以内的向量还是容易理解和想象。\n",
    "\n",
    "但是更高维度的向量可以具象成什么样子人类是很难想象的。\n",
    "所以我们在理解高维度向量的时候可以跳出想象的限制，直接从概念入手，100维的向量就是100个元素的数组。从这个角度理解的话，以后要理解无限维度向量这样的概念就会比较简单。\n",
    "\n",
    "\n",
    "<div style=\"display:flex; justify-content:space-between; max-width:700px;\">\n",
    "  <img src=\"../images/001_axis_and_vector.png\" alt=\"二维向量\" style=\"max-width:48%;max-height:300px;background-color:#333333;\">\n",
    "  <img src=\"../images/001_axis_and_3dvector.png\" alt=\"三维向量\" style=\"max-width:48%;max-height:300px;background-color:#333333;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 人造的神经元\n",
    "\n",
    "现在的深度学习技术是受人类大脑的神经元结构的启发而来的。虽然大家都知道深度学习的工作方式和人类大脑是不一样的，在很长一段时间内大家都接受这个事实，觉得不需要做得和人脑一样强。我们让深度学习可以完成一些比以前复杂的任务就行，不需要比人类强。\n",
    "直到有一天大语言模型（LLM）出现，大家才发现，说不定现在深度学习的方法比人脑的方法更好。\n",
    "\n",
    "深度学习里面的最基本结构是一个 人造神经元 ， 或者用学术名词叫 “感知器”，英文是Perceptron。\n",
    "著名的人工智能科学家 马文.明斯基 （Marvin Minsky）就曾经以 “Perceptron” 为书名写过一本专门分析“感知器”的书，书中还讨论了“感知器”的限制，就是单个“感知器”不能解决“异或问题”，甚至有人认为是这本书导致了人工智能第一次寒冬。\n",
    "关于“异或问题”我们后面再讨论，现在先看看一个 人造神经元 是什么样的。\n",
    "\n",
    "人造神经元的基本思路就是拿到一堆输入值，需要计算出来一个合适的输出值，一堆输入值对应的就是神经元的树突，一个输出值就对应神经元的轴突，计算逻辑对应神经元主体。\n",
    "\n",
    "一堆输入值的存储就用到了上面讨论的第一个基础概念：向量。如果有n个输入值，我们就用n维向量来存储和表达，输出值只有一个，可以用一个标量来表示，或者用一个只有一个元素的一维向量来存储。\n",
    "\n",
    "于是有了以下的关系：\n",
    "\n",
    "\\begin{align*}\n",
    " \\begin{bmatrix} x_1 & x_2 & \\cdots & x_n \\end{bmatrix}   ？=    y_1 \n",
    "\\end{align*}\n",
    "\n",
    "我们要做的就是找到一个合适的方式形成这种关系。一个合理而简单的方式就是给每一个$x_i$ 配一个权重 $w_i$，然后把所有$x_i * w_i$ 加起来当作是$y_1$。\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "  x_1*w_1  + x_2*w_2 + x_3*w_3 + ... + x_n*w_n = y_1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "用严谨一点的数学表示是这样：\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix}   . \n",
    " \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\n",
    " =    y_1 \n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "两个向量 w 和 x之间的 . 符号是点乘的意思，简单理解就是把两个向量对应位置的元素拿出来相乘，然后把乘积都加起来。\n",
    "就是上面的$x_1*w_1  + x_2*w_2 + x_3*w_3 + ... + x_n*w_n$\n",
    "\n",
    "还有很多地方会用Σ符号来表示：\n",
    "\n",
    " $$\\sum_{i=1}^{n} x_i w_i$$\n",
    " \n",
    " 甚至偷懒写成下面这样：\n",
    " \n",
    " \n",
    "$$\\sum  w_n  x_n$$\n",
    "\n",
    "如果直接用向量符号就是这样：\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{w}^T  \\cdot \\vec{x} \n",
    "\\end{align}\n",
    "\n",
    "或者再简化一点，直接用大写字母，不要头上的向量符号：\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "W^T  \\cdot X \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "用常见的神经网络示意图就是如下这个图的样子，以n=4为例：\n",
    "\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "        <img src=\"../images/001_neuron.png\" alt=\"人造神经元\" style=\"max-height:400px;background-color:#333333;\">\n",
    "        </div>\n",
    "\n",
    "\n",
    "这里还有一个细节需要注意，一般的线性映射函数都会有一个叫偏置项的输入，常用字母$b$表示 \n",
    "表达成这样，这里面所谓的偏置项就是我们常说的常数项，就是和输入x无关的一个项：\n",
    "\n",
    "\\begin{align}\n",
    "W^T  \\cdot X  + b\n",
    "\\end{align}\n",
    "\n",
    "我们以前学习基本的数字知识的时候用的一次函数里也会提这个常数项，比如：\n",
    "\n",
    "\\begin{align}\n",
    "y = ax + b\n",
    "\\end{align}\n",
    "\n",
    "这里的$a$是一次项，$b$是常数项，这里的$b$和上面的$b$表达的是同一个意思。\n",
    "\n",
    "在实际实现中，为了方便，经常的做法就是加一个$x_0$，让它等于1， 那这样$w_0$就相当于$b$了\n",
    "\n",
    "图示如下：\n",
    "\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "        <img src=\"../images/001_neuron_with_b.png\" alt=\"人造神经元\" style=\"max-height:400px;background-color:#333333;\">\n",
    "        </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 最简单的神经网络\n",
    "\n",
    "\\begin{align}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 最简单的神经网络\n",
    "\n",
    "### 1. 单层神经网络\n",
    "\n",
    "单层神经网络由多个人造神经元组成。每个人造神经元接收输入向量 X，并通过权重向量 W 和偏置项 b 进行加权求和运算,最后通过激活函数 f 得到输出。数学表达式如下:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "y = f(W^T \\cdot X + b)\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- X 是输入向量,维度为 n-1 （使用n-1是为了给偏置项b留一个维度，后面把偏置项算上就是n维）\n",
    "- W 是权重向量,维度为 n-1 （使用n-1是为了给偏置项b留一个维度，后面把偏置项算上就是n维）\n",
    "- b 是偏置项,是一个标量\n",
    "- f 是激活函数,如 sigmoid、ReLU 等\n",
    "\n",
    "我们现在还没有讨论为什么要有激活函数，所以可以先从简化的角度把激活函数去掉。\n",
    "\n",
    "然后上文讨论过，偏置项b可以通过设计恒为1的$x_0$直接放进$W$中\n",
    "\n",
    "于是有了这样的表达：\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "y = W^T \\cdot X \n",
    "\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- X 是输入向量,维度为 n\n",
    "- W 是权重向量,维度为 n\n",
    "\n",
    "\n",
    "\n",
    "将多个人造神经元组合在一起,就形成了单层神经网络。假设有 m 个神经元,输出向量 Y 的维度为 m,则单层神经网络可表示为:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "Y = W^T \\cdot X\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- X 是输入向量,维度为 n\n",
    "- W 是权重矩阵,维度为 m × n\n",
    "\n",
    "形成的单层神经网络可以用以下示意图表达：\n",
    "\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "    <img src=\"../images/001_network_single_layer.png\" alt=\"单层神经网络\" style=\"max-height:300px;background-color:#333333;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "如果要强调有偏置项$B$，也可以加上$x_0$和$w_0$，表达成这样：\n",
    "\n",
    "以后的讨论中我们就不再强调这一点，有没有偏置项只是处理的一个细节。\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "    <img src=\"../images/001_network_single_layer_with_b.png\" alt=\"单层神经网络\" style=\"max-height:300px;background-color:#333333;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### 2. 多层神经网络\n",
    "\n",
    "多层神经网络由多个单层神经网络组成,每一层的输出作为下一层的输入。假设有 L 层,第 l 层的输出 Y^(l) 可表示为:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "Y^{(l)} = f^{(l)}(W^{(l)T} \\cdot Y^{(l-1)} + B^{(l)})\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "其中:\n",
    "- Y^(l-1) 是第 l-1 层的输出,也是第 l 层的输入\n",
    "- W^(l) 是第 l 层的权重矩阵\n",
    "- B^(l) 是第 l 层的偏置向量\n",
    "- f^(l) 是第 l 层的激活函数\n",
    "\n",
    "同样，简化一下，先不考虑偏置项和激活函数，可以表示为：\n",
    "\n",
    "假设有 L 层,第 l 层的输出 Y^(l) 可表示为:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "Y^{(l)} = W^{(l)T} \\cdot Y^{(l-1)} \n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "其中:\n",
    "- Y^(l-1) 是第 l-1 层的输出,也是第 l 层的输入\n",
    "- W^(l) 是第 l 层的权重矩阵\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "对于整个多层神经网络,输入为 X,输出为 Y^(L),中间层的计算过程为:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "Y^{(1)} &= W^{(1)T} \\cdot X \\\\\n",
    "Y^{(2)} &= W^{(2)T} \\cdot Y^{(1)} \\\\\n",
    "&\\vdots \\\\\n",
    "Y^{(L)} &= W^{(L)T} \\cdot Y^{(L-1)} \n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "其中，$Y^{(1)}$ 到 $Y^{(L)}$ 都是向量，而且他们的维度可以不同。\n",
    "\n",
    "一下的图示展示了一个2层的神经网络，第一层有3个神经元，第二层有5个神经元。\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "    <img src=\"../images/001_network.png\" alt=\"多层神经网络\" style=\"max-height:300px;background-color:#333333;\">\n",
    "</div>\n",
    "\n",
    "通过添加更多隐藏层和神经元,多层神经网络可以拟合更加复杂的函数,从而解决更加困难的问题。\n",
    "\n",
    "另外有一个非常重要的事情，我们现在还没有讨论权重$W$和偏置量$B$如何才能正确设置,这点我们在下一个大章节关于神经网络的训练的时候详细展开。\n",
    "\n",
    "在讨论神经网络的训练过程之前，我们先假定我们已经可以学习到一组合适的权重$W$和偏置量$B$并设置到神经网络中。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要激活函数（线性坍塌带来的问题）\n",
    "\n",
    "在没有激活函数的情况下，多层神经网络会退化成单层神经网络，这种现象被称为线性坍塌。我们以一个两层神经网络为例来说明这个问题。\n",
    "\n",
    "这个神经网络的示意图如下：\n",
    "\n",
    "\n",
    "<div style=\"align-items: center;\">\n",
    "    <img src=\"../images/001_linear_collapse.png\" alt=\"线性坍塌\" style=\"max-height:300px;background-color:#333333;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "假设第一层有两个输入 x1 和 x2，通过两个加权求和运算得到 h1 和 h2:\n",
    "\n",
    "\\begin{align}\n",
    "h_1 = ax_1 + bx_2 + c\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "h_2 = ex_1 + fx_2 + g\n",
    "\\end{align}\n",
    "\n",
    "其中 $a,b,e,f$ 表示第一层的权重矩阵元素，$c,g$ 表示第一层的偏置向量元素。\n",
    "\n",
    "然后，h1 和 h2 通过另一个加权求和运算得到输出 y:\n",
    "\n",
    "\\begin{align}\n",
    "y = uh_1 + vh_2 + z\n",
    "\\end{align}\n",
    "\n",
    "其中 $u, v$ 表示第二层的权重向量元素，$z$ 表示第二层的偏置项。\n",
    "\n",
    "将上面的公式代入 y 的表达式中，我们可以得到:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "y &= u(ax_1 + bx_2 + c) + v(ex_1 + fx_2 + g) + z \\\\\n",
    "y &= (ua + ve)x_1 + (ub + vf)x_2 + uc + vg + z \\\\\n",
    "y &= w_1x_1 + w_2x_2 + b\n",
    "\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "可以看到，上面的表达式实际上等价于一个单层神经网络:\n",
    "\n",
    "\n",
    "其中:\n",
    "\n",
    "\\begin{align}\n",
    "w_1 = ua + ve\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "w_2 = ub + vf\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "b = uc + vg + z\n",
    "\\end{align}\n",
    "\n",
    "因此，在没有激活函数的情况下，多层神经网络会退化成单层神经网络，失去了多层网络的表达能力。这就是线性坍塌的问题。引入非线性激活函数可以很好地解决这个问题，使得多层神经网络具有更强的表达能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Python代码,使用NumPy和Matplotlib来演示两个线性函数相加的结果仍然是一个线性函数.\n",
    "\n",
    "这是一个可以互动的动态图表，读者可以调整a,b,c,d的数值来观察各条线的变化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada9281c69e94db589e7dc2dcef21965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='a', min=-100.0, step=1.0), FloatSlider(value=30.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(a, b, d, e)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "\n",
    "def interactive_plot(a, b, d, e):\n",
    "\n",
    "    # 生成 x 的样本\n",
    "    x = np.linspace(-100, 100, 100)\n",
    "\n",
    "    # 计算 y1, y2, y3\n",
    "    y1 = a * x + b\n",
    "    y2 = d * x + e\n",
    "    y3 = y1 + y2\n",
    "\n",
    "    # 绘制图形\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    #set the maximum and minimum value of x and y\n",
    "    \n",
    "    plt.xlim(-100, 100)\n",
    "    plt.ylim(-100, 100)\n",
    "    \n",
    "    plt.plot(x, y1, label='y1 = ax + b')\n",
    "    plt.plot(x, y2, label='y2 = dx + e')\n",
    "    plt.plot(x, y3, label='y3 = y1 + y2')\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Linear Function Addition')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "interact(interactive_plot, \n",
    "         a=FloatSlider(min=-100., max=100., step=1, value=2, description='a'), \n",
    "         b=FloatSlider(min=-100., max=100., step=1, value=30, description='b'),\n",
    "         d=FloatSlider(min=-100., max=100., step=1, value=-1, description='c'),\n",
    "         e=FloatSlider(min=-100., max=100., step=1, value=15, description='d'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在上面的代码中,我们首先定义了两个线性函数的参数 a, b 和 d, e。然后,我们使用 `np.linspace` 函数生成了一个从 -100 到 100 的等间隔样本 x,包含 100 个点。\n",
    "\n",
    "接下来,我们计算了两个线性函数 y1 和 y2 的值,以及它们的和 y3。具体来说:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "y_1 = ax + b = 2x + 30\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y_2 = dx + e = -x + 15\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y_3 = y_1 + y_2 = (a + d)x + (b + e) = x + 45\n",
    "\\end{align}\n",
    "可以看到,y3 的表达式仍然是一个线性函数,其中系数为 a + d = 1,常数项为 b + e = 45。\n",
    "\n",
    "最后,我们使用 Matplotlib 库绘制了三条线性函数在给定 x 范围内的曲线,并添加了图例、坐标轴标签和标题。\n",
    "\n",
    "运行上述代码后,我们可以在绘制的图形中清晰地看到,y3 的曲线就是 y1 和 y2 曲线的点wise相加的结果,并且 y3 本身也是一条直线,证明了两个线性函数相加的结果仍然是一个线性函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数介绍\n",
    "\n",
    "激活函数是神经网络中非常重要的一个组成部分,它引入了非线性,使得神经网络能够拟合更加复杂的函数。下面我们介绍几种常用的激活函数。\n",
    "\n",
    "### Sigmoid 函数\n",
    "\n",
    "Sigmoid 函数的数学表达式为:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}\n",
    "\n",
    "它将输入值映射到了 (0, 1) 范围内,具有平滑且可导的性质。然而,Sigmoid 函数在正负较大的输入值时,导数接近于 0,这会导致梯度消失的问题,从而使得神经网络在训练过程中收敛缓慢。\n",
    "\n",
    "### Tanh 函数\n",
    "\n",
    "Tanh 函数的数学表达式为:\n",
    "\n",
    "\\begin{align}\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "\\end{align}\n",
    "\n",
    "它将输入值映射到了 (-1, 1) 范围内,相比于 Sigmoid 函数,它是一种零均值的函数,收敛速度更快。但是,它也存在梯度消失的问题。\n",
    "\n",
    "### ReLU 函数\n",
    "\n",
    "ReLU (Rectified Linear Unit) 函数的数学表达式为:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align}\n",
    "\n",
    "它是一种分段线性函数,在正半轴上是线性的,在负半轴上是 0。ReLU 函数的优点包括:\n",
    "\n",
    "1. 计算简单,只需判断输入是否大于 0\n",
    "2. 不存在梯度消失问题,当 x > 0 时,导数为 1\n",
    "3. 提供了稀疏表达,使得网络更加高效\n",
    "\n",
    "然而,ReLU 函数也存在一些问题:\n",
    "\n",
    "1. 死亡神经元问题:当输入为负值时,导数为 0,神经元将永远不会被激活\n",
    "2. 非平滑,在 x = 0 处不可导\n",
    "\n",
    "针对这些问题,研究者提出了一些变体,如 Leaky ReLU、PReLU 等,它们在负半轴上保留了一个较小的非零斜率,从而缓解了死亡神经元问题,同时也使得函数在 x = 0 处可导。\n",
    "\n",
    "此外,还有一些其他的激活函数,如 Maxout、ELU 等,它们各有特点,在不同的场景下表现不同。选择合适的激活函数对于神经网络的性能至关重要。\n",
    "\n",
    "以下是对上文线性坍塌演示代码的修改，加入了ReLU激活函数，展示激活函数的效果，这个也是一个动图演示图，读者可以调整a,b,c,d的数值对结果进行观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001c5d812fe04bc091634af38d1e54e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='a', max=10.0, min=-10.0, step=1.0), FloatSlider(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(a, b, d, e)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "\n",
    "def interactive_plot(a, b, d, e):\n",
    "\n",
    "    # 生成 x 的样本\n",
    "    x = np.linspace(-100, 100, 100)\n",
    "\n",
    "    # 计算 y1, y2, y3， 这里用 np 函数库里的 maximum 函数实现 ReLU 函数\n",
    "    y1 = np.maximum(0, a * x + b)\n",
    "    y2 = np.maximum(0, d * x + e)\n",
    "    y3 = y1 + y2\n",
    "\n",
    "    # 绘制图形\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    #set the maximum and minimum value of x and y\n",
    "    \n",
    "    plt.xlim(-100, 100)\n",
    "    plt.ylim(-100, 100)\n",
    "    \n",
    "    plt.plot(x, y1, label='y1 = ReLU(ax + b)')\n",
    "    plt.plot(x, y2, label='y2 = ReLU(dx + e)')\n",
    "    plt.plot(x, y3, label='y3 = y1 + y2')\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Linear Function Addition')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "interact(interactive_plot, \n",
    "         a=FloatSlider(min=-10., max=10., step=1, value=2, description='a'), \n",
    "         b=FloatSlider(min=-100., max=100., step=1, value=30, description='b'),\n",
    "         d=FloatSlider(min=-10., max=10., step=1, value=-1, description='c'),\n",
    "         e=FloatSlider(min=-100., max=100., step=1, value=15, description='d'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在修改后的代码中,我们使用了 NumPy 中的 `np.maximum` 函数来实现 ReLU 激活函数。具体来说:\n",
    "\n",
    "\\begin{align}\n",
    "y_1 = \\text{ReLU}(ax + b) = \\max(0, ax + b)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y_2 = \\text{ReLU}(dx + e) = \\max(0, dx + e)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y_3 = y_1 + y_2\n",
    "\\end{align}\n",
    "\n",
    "由于 ReLU 函数引入了非线性,因此 y3 的表达式不再是一个线性函数。相反,它是两个分段线性函数的叠加。\n",
    "\n",
    "在绘制图形时,我们将 y1 和 y2 的标签分别更新为 `'y1 = ReLU(2x + 30)'` 和 `'y2 = ReLU(-x + 15)'`,以反映它们现在是 ReLU 激活函数的输出。同时,图形标题也更新为 `'ReLU Function Addition'`。\n",
    "\n",
    "运行修改后的代码,我们可以看到,y3 的曲线不再是一条直线,而是由多个线段组成的分段线性函数。这说明,当线性函数经过非线性激活函数的变换后,它们相加的结果不再是线性函数。\n",
    "\n",
    "通过这个例子,我们可以直观地看到,非线性激活函数在神经网络中的重要作用。它使得神经网络能够拟合更加复杂的函数,从而提高了模型的表达能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
