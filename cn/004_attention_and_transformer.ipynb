{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的,根据你提供的大纲,我将为第四节生成相应的内容。\n",
    "\n",
    "## 4. 深入了解 Attention 和 Transformer\n",
    "\n",
    "### 4.1 Query Key Value\n",
    "\n",
    "注意力机制的核心思想是通过 Query 查询与 Key 的相关性,对 Value 进行加权求和。其中 Query、Key 和 Value 可以是不同的表示,也可以是相同的表示。这种机制赋予了模型选择性地聚焦输入的能力。\n",
    "\n",
    "具体来说,注意力机制的计算过程如下:\n",
    "\n",
    "1. 计算 Query 与所有 Key 的相似性得分,通常使用点积或缩放点积:\n",
    "   \\begin{align}\n",
    "   \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "   \\end{align}\n",
    "   其中 $d_k$ 是 Key 的维度,用于缩放点积以防止过大或过小的值。\n",
    "\n",
    "2. 对相似性得分进行 softmax 操作,得到注意力分数向量 $\\alpha$。\n",
    "\n",
    "3. 将注意力分数向量 $\\alpha$ 与 Value 进行加权求和,得到注意力加权的表示 $z$:\n",
    "   \\begin{align}\n",
    "   z = \\sum_{i=1}^{n} \\alpha_i V_i\n",
    "   \\end{align}\n",
    "\n",
    "通过注意力机制,模型可以自适应地聚焦输入的不同部分,捕获更有价值的信息,从而提高模型的性能。在不同的应用场景中,Query、Key 和 Value 的具体含义可能不同,但基本原理是相同的。\n",
    "\n",
    "### 4.2 位置编码\n",
    "\n",
    "由于 Transformer 没有递归结构,因此需要一种方式来注入序列的位置信息。位置编码就是将序列的位置信息编码为向量,并与序列的表示相加,从而使得模型能够捕获元素在序列中的相对位置和顺序。\n",
    "\n",
    "位置编码的计算公式如下:\n",
    "\n",
    "\\begin{align}\n",
    "PE_{(pos, 2i)} &= \\sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos(pos / 10000^{2i/d_{\\text{model}}})\n",
    "\\end{align}\n",
    "\n",
    "其中 $pos$ 是位置索引, $i$ 是维度索引, $d_{\\text{model}}$ 是模型的维度。通过使用不同频率的正弦和余弦函数,位置编码可以为每个位置生成一个唯一的向量表示。\n",
    "\n",
    "将位置编码与序列的表示相加,可以得到包含位置信息的表示:\n",
    "\n",
    "\\begin{align}\n",
    "X' = X + PE\n",
    "\\end{align}\n",
    "\n",
    "其中 $X$ 是原始的序列表示, $X'$ 是加入位置信息后的表示。\n",
    "\n",
    "通过位置编码,Transformer 可以捕获序列中元素的位置和顺序信息,从而更好地建模序列数据。\n",
    "\n",
    "### 4.3 Transformer 编码器\n",
    "\n",
    "Transformer 的编码器由多个相同的层组成,每层包含多头注意力子层和前馈神经网络子层。\n",
    "\n",
    "**多头注意力子层**对输入序列进行自注意力编码,捕获序列中元素之间的依赖关系。具体来说,它将输入 $X$ 线性映射为 Query、Key 和 Value,然后进行注意力计算:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, \\dots, head_h)W^O\n",
    "\\end{align}\n",
    "\n",
    "其中 $head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$, $W_i^Q$、$W_i^K$、$W_i^V$ 和 $W^O$ 是可学习的线性映射参数。通过多头注意力,模型可以从不同的子空间捕获不同的依赖关系。\n",
    "\n",
    "**前馈神经网络子层**对每个位置的表示进行独立的非线性变换,增强了模型的表达能力。它的计算过程如下:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "\\end{align}\n",
    "\n",
    "其中 $W_1$、$W_2$、$b_1$、$b_2$ 是可学习的参数。\n",
    "\n",
    "编码器的输出将作为解码器的输入,用于生成目标序列。\n",
    "\n",
    "### 4.4 Transformer 解码器\n",
    "\n",
    "解码器与编码器类似,也由多头注意力子层和前馈神经网络子层组成,但增加了一个对编码器输出序列的注意力子层,以及遮挡未来位置的遮挡注意力机制,使得预测只依赖于当前及之前的输出。\n",
    "\n",
    "**遮挡注意力机制**确保在生成序列的每个位置时,只依赖于该位置之前的输出。它通过在注意力计算中引入一个遮挡掩码,将未来位置的注意力分数设置为一个很小的负值(如 -1e9),从而忽略未来位置的影响。\n",
    "\n",
    "**编码器-解码器注意力子层**通过注意力机制,将解码器的输出与编码器的输出进行关联。具体来说,它将解码器的输出作为 Query,编码器的输出作为 Key 和 Value,计算注意力加权的表示,并将其与解码器的输出相加。\n",
    "\n",
    "通过上述机制,解码器可以有效地利用编码器捕获的输入序列信息,同时避免未来信息的泄露,从而生成高质量的目标序列。\n",
    "\n",
    "### 4.5 Encoder-Decoder 架构\n",
    "\n",
    "Encoder-Decoder 架构常用于序列到序列的任务,如机器翻译。在这种架构中,编码器将输入序列编码为中间表示,解码器则基于该表示生成输出序列。两者之间通过注意力机制建立联系。\n",
    "\n",
    "具体来说,编码器首先将输入序列 $X = (x_1, x_2, \\dots, x_n)$ 编码为一系列向量表示 $H = (h_1, h_2, \\dots, h_n)$,捕获输入序列的重要信息。然后,解码器在生成每个输出元素 $y_t$ 时,会计算注意力加权的表示 $c_t$,将编码器的输出 $H$ 与当前的解码器状态 $s_t$ 进行关联:\n",
    "\n",
    "\\begin{align}\n",
    "c_t = \\text{Attention}(s_t, H, H)\n",
    "\\end{align}\n",
    "\n",
    "最终,解码器根据注意力加权的表示 $c_t$ 和当前状态 $s_t$ 生成输出 $y_t$:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_t | y_1, \\dots, y_{t-1}, X) = g(y_{t-1}, s_t, c_t)\n",
    "\\end{align}\n",
    "\n",
    "其中 $g$ 是一个非线性函数,通常由前馈神经网络或循环神经网络实现。\n",
    "\n",
    "Encoder-Decoder 架构在机器翻译、文本摘要等任务中表现出色,成为序列到序列建模的主流方法之一。\n",
    "\n",
    "### 4.6 Decoder-only 架构\n",
    "\n",
    "Decoder-only 架构常用于生成任务,如文本生成。在这种架构中,模型的输入和输出都是文本序列,解码器需要对输入序列进行自回归建模,预测每个位置的标记仅依赖于之前的标记。\n",
    "\n",
    "具体来说,给定输入序列 $X = (x_1, x_2, \\dots, x_n)$,解码器需要生成目标序列 $Y = (y_1, y_2, \\dots, y_m)$。在生成每个输出元素 $y_t$ 时,解码器会计算注意力加权的表示 $c_t$,将输入序列 $X$ 与当前的解码器状态 $s_t$ 进行关联:\n",
    "\n",
    "\\begin{align}\n",
    "c_t = \\text{Attention}(s_t, X, X)\n",
    "\\end{align}\n",
    "\n",
    "然后,解码器根据注意力加权的表示 $c_t$、当前状态 $s_t$ 和上一个输出 $y_{t-1}$ 生成输出 $y_t$:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_t | y_1, \\dots, y_{t-1}, X) = g(y_{t-1}, s_t, c_t)\n",
    "\\end{align}\n",
    "\n",
    "其中 $g$ 是一个非线性函数,通常由前馈神经网络或循环神经网络实现。\n",
    "\n",
    "在 Decoder-only 架构中,解码器需要通过自注意力机制捕获输入序列的依赖关系,同时利用遮挡注意力机制避免未来信息的泄露。这种架构常用于文本生成、对话系统等任务,成为了生成式建模的重要方法之一。\n",
    "\n",
    "通过上述内容,我们深入了解了 Attention 机制和 Transformer 架构的核心原理和关键组件。Attention 机制赋予了模型选择性地聚焦输入的能力,而 Transformer 则通过完全基于注意力机制的设计,实现了高效的并行计算和长期依赖建模。这些创新为自然语言处理、计算机视觉等领域带来了重大突破,推动了深度学习的发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
