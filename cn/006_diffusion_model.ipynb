{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 扩散模型\n",
    "\n",
    "### 6.1 扩散模型基本结构\n",
    "\n",
    "扩散模型(Diffusion Models)是一种新兴的生成模型,近年来在图像、音频和视频生成等领域取得了卓越的成绩。扩散模型的基本思想最早来自于2020年的论文[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239),该论文提出了扩散模型的基本框架,包括正向扩散过程和反向生成过程,为扩散模型奠定了理论基础。\n",
    "\n",
    "扩散模型由两个主要部分组成:一个正向扩散过程(forward diffusion process)和一个反向过程(reverse process)。\n",
    "\n",
    "**正向扩散过程**的目标是将原始数据(如图像或音频)逐步破坏,加入高斯噪声,直到完全变为纯噪声。这个过程可以用一个马尔可夫链来描述,每一步都会将数据向纯噪声状态推进一小步。具体来说,给定一个初始数据 $x_0$,正向扩散过程会生成一系列逐步加噪的数据 $x_1, x_2, \\dots, x_T$,其中 $x_T$ 是纯噪声。\n",
    "\n",
    "**反向过程**的目标则是从噪声数据 $x_T$ 出发,通过训练的神经网络逐步去噪,重建出原始数据 $x_0$。这个过程由一个生成模型 $p_\\theta(x_{t-1}|x_t)$ 参数化,其中 $\\theta$ 是神经网络的参数。生成模型会学习从噪声数据 $x_t$ 中去除噪声,生成更清晰的数据 $x_{t-1}$,直到最终生成出原始数据 $x_0$。\n",
    "\n",
    "扩散模型的这种结构使得模型能够从低分辨率开始,逐步生成高分辨率的图像或音频等数据,而不需要像GAN那样直接生成高分辨率数据。这种逐步生成的方式有助于捕捉数据的细节和结构信息,从而提高生成质量。\n",
    "\n",
    "### 6.2 CLIP\n",
    "\n",
    "CLIP(Contrastive Language-Image Pre-training)是一种用于图像-文本对比任务的模型,由OpenAI在2021年提出,相关论文为[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)。它通过对大量图像-文本对进行对比式训练,学习将图像和文本映射到同一个潜在空间,从而实现跨模态的对比和检索。\n",
    "\n",
    "CLIP是一种基于对比学习的视觉-语言预训练模型,主要工作原理如下:\n",
    "\n",
    "1. 预训练数据集:构建了一个包含4亿张图像-文本对的大规模数据集,其中图像来自互联网,文本是与图像相关的描述性文字。\n",
    "\n",
    "2. 模型架构:CLIP由一个图像编码器(如ResNet或Vision Transformer)和一个文本编码器(Transformer)组成。\n",
    "\n",
    "3. 预训练目标:CLIP的预训练目标是最大化一个batch内真实图像-文本对的相似性得分,同时最小化错误配对的相似性得分。具体来说,对于一个batch内的N个图像-文本对,模型需要从N^2种可能的配对中预测出正确的N个配对。\n",
    "\n",
    "4. 无监督对比学习:CLIP通过对比学习的方式,在不需要人工标注的情况下,学习到一个跨模态的embedding空间,使得每个图像和与之对应的文本描述在该空间中的表示相近。\n",
    "\n",
    "5. 零样本迁移:在预训练完成后,CLIP可以通过将任意的文本描述映射到embedding空间,从而生成一个零样本分类器,实现对各种视觉任务的零样本迁移。\n",
    "\n",
    "\n",
    "\n",
    "CLIP为扩散模型提供了强大的文本条件能力,使得扩散模型可以根据文本提示生成相应的图像或其他模态数据,极大地扩展了生成的灵活性和可控性。\n",
    "\n",
    "### 6.3 U-Net\n",
    "\n",
    "U-Net是一种广泛应用于图像分割任务的卷积网络架构,由Olaf Ronneberger等人于2015年在论文[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)中提出。它采用对称的编码器-解码器结构,使用跨层连接来融合不同分辨率的特征,从而提高模型的性能。\n",
    "\n",
    "由于U-Net能够有效地融合不同分辨率的特征,因此它常被用作扩散模型的主干网络,用于从噪声中重建图像。在扩散模型中,U-Net的编码器部分用于从噪声数据中提取特征,而解码器部分则根据这些特征生成清晰的图像。跨层连接有助于保留图像的细节信息,提高生成质量。\n",
    "\n",
    "### 6.4 潜空间\n",
    "\n",
    "潜空间(Latent Space)指的是模型学习到的低维连续表示空间,能够对原始高维数据(如图像)进行压缩和解压缩。扩散模型通过在潜空间中进行采样和推理,实现了对原始数据的高效生成。这一概念源自自编码器和生成对抗网络等模型,如论文[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)和[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)。\n",
    "\n",
    "具体来说,扩散模型将原始数据 $x_0$ 映射到一个低维潜空间 $z$,即 $z = f(x_0)$,其中 $f$ 是一个编码器网络。在潜空间中,模型可以通过采样或操作来获得新的潜在表示 $z'$,然后通过一个解码器网络 $g$ 将其解码为生成数据 $x' = g(z')$。\n",
    "\n",
    "潜空间为扩散模型提供了一种紧凑且富有语义的数据表示方式,极大地提高了模型的生成能力和可控性。\n",
    "\n",
    "### 6.5 空间patch和时空patch\n",
    "\n",
    "在处理图像或视频等具有空间/时空结构的数据时,可以将其划分为多个patch(图像块或视频clip),并在patch级别上进行建模和生成,这有助于捕捉局部结构信息。相关论文包括[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)和[Video Diffusion Models](https://arxiv.org/abs/2204.03458)。\n",
    "\n",
    "**空间patch**指的是将图像划分为多个小块,每个小块被视为一个patch。通过在patch级别上进行建模,模型可以更好地捕捉图像的局部结构和纹理信息,从而提高生成质量。\n",
    "\n",
    "**时空patch**则是将视频进一步划分为多个短时间片段(clip),每个clip包含若干连续的帧,被视为一个时空patch。通过在时空patch级别上进行建模,模型不仅可以捕捉空间结构信息,还能捕捉时间上的动态信息,从而更好地生成动态视频。\n",
    "\n",
    "在扩散模型中,常见的做法是将输入数据划分为多个patch,然后对每个patch进行独立的扩散和生成过程。通过这种局部化的建模方式,模型可以更好地关注局部结构,提高生成质量。同时,patch划分还可以带来并行计算、条件生成等优势,增强了模型的灵活性和计算效率。\n",
    "\n",
    "通过上述关键组件的介绍,我们全面了解了扩散模型的基本原理、重要模块以及相关技术细节。这些基础工作为扩散模型的发展奠定了坚实的理论和技术基础,促进了该领域的快速进展。相信在未来,扩散模型及其相关技术将会带来更多令人兴奋的突破和进展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
