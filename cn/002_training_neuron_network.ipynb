{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的,根据你提供的大纲,我将为第二节生成相应的内容。\n",
    "\n",
    "## 2. 训练一个神经网络\n",
    "\n",
    "### 2.1 误差\n",
    "\n",
    "在训练神经网络时,我们需要定义一个目标函数,也称为损失函数或代价函数,用于量化神经网络的输出与期望输出之间的差异。通过最小化这个目标函数,我们可以不断调整神经网络的参数,使得输出逐渐接近期望值。\n",
    "\n",
    "常用的目标函数包括:\n",
    "\n",
    "1. **均方误差 (Mean Squared Error, MSE)**\n",
    "\n",
    "均方误差是回归问题中常用的目标函数,它计算预测值与真实值之间的平方差的平均值。对于一个样本,均方误差可以表示为:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "\\end{align}\n",
    "\n",
    "其中 $n$ 是样本数量, $y_i$ 是第 $i$ 个样本的真实值, $\\hat{y}_i$ 是第 $i$ 个样本的预测值。\n",
    "\n",
    "2. **交叉熵损失 (Cross-Entropy Loss)**\n",
    "\n",
    "交叉熵损失常用于分类问题,它度量了预测概率分布与真实概率分布之间的差异。对于一个样本,交叉熵损失可以表示为:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{CE} = -\\sum_{i=1}^{C}y_i\\log(\\hat{y}_i)\n",
    "\\end{align}\n",
    "\n",
    "其中 $C$ 是类别数量, $y_i$ 是第 $i$ 类的真实标签 (0 或 1), $\\hat{y}_i$ 是第 $i$ 类的预测概率。\n",
    "\n",
    "通过最小化这些目标函数,我们可以使神经网络的输出逐渐接近期望值。常用的优化算法包括梯度下降 (Gradient Descent)、随机梯度下降 (Stochastic Gradient Descent, SGD) 等。\n",
    "\n",
    "### 2.2 反向传播\n",
    "\n",
    "反向传播 (Backpropagation) 是训练多层神经网络的核心算法,它通过链式法则计算每个权重对误差的贡献,从而确定梯度方向,指导权重的调整。\n",
    "\n",
    "具体来说,反向传播算法包括以下步骤:\n",
    "\n",
    "1. **前向传播**: 输入样本通过神经网络层层传递,计算每一层的输出。\n",
    "2. **计算输出层误差**: 将输出层的输出与真实标签进行比较,计算输出层的误差。\n",
    "3. **反向传播误差**: 根据链式法则,将输出层的误差反向传播到前一层,计算每一层的误差。\n",
    "4. **计算梯度**: 对每一层的权重和偏置计算梯度,即它们对误差的偏导数。\n",
    "5. **更新权重和偏置**: 根据梯度的方向,使用优化算法 (如梯度下降) 更新权重和偏置。\n",
    "\n",
    "反向传播算法使得深层神经网络可以被有效地训练,因为它提供了一种端到端的误差传递机制,使得每一层的参数都可以根据最终的输出误差进行调整。\n",
    "\n",
    "### 2.3 偏导数\n",
    "\n",
    "偏导数是反向传播算法的基础,它用于计算某个权重或偏置对误差的影响程度。通过对网络输出关于每个权重和偏置的偏导数,我们可以确定梯度的方向,从而指导参数的调整。\n",
    "\n",
    "假设我们有一个单层神经网络,输入为 $\\vec{x}$,权重为 $\\vec{w}$,偏置为 $b$,激活函数为 $f$,输出为 $y$,真实标签为 $t$,损失函数为 $L$。那么,我们可以计算权重 $w_i$ 对损失函数的偏导数:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial y}\\cdot\\frac{\\partial y}{\\partial w_i}\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial y}$ 是损失函数关于输出的偏导数,可以直接计算得到。\n",
    "- $\\frac{\\partial y}{\\partial w_i} = \\frac{\\partial f(\\vec{w}^T\\vec{x} + b)}{\\partial w_i} = f'(\\vec{w}^T\\vec{x} + b)\\cdot x_i$,利用链式法则计算。\n",
    "\n",
    "同理,我们可以计算偏置 $b$ 对损失函数的偏导数:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}\\cdot\\frac{\\partial y}{\\partial b} = \\frac{\\partial L}{\\partial y}\\cdot f'(\\vec{w}^T\\vec{x} + b)\n",
    "\\end{align}\n",
    "\n",
    "通过计算这些偏导数,我们可以确定梯度的方向,并使用优化算法 (如梯度下降) 更新权重和偏置。\n",
    "\n",
    "### 2.4 链式法则\n",
    "\n",
    "链式法则是计算复合函数导数的一种技巧,在反向传播中用于计算深层网络中的梯度。它将复合函数的导数分解为内部子函数导数的乘积,使得梯度计算可以遍历整个网络层次结构。\n",
    "\n",
    "假设我们有一个复合函数 $y = f(g(x))$,其中 $f$ 和 $g$ 都是可导函数。根据链式法则,我们可以计算 $y$ 关于 $x$ 的导数:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dy}{dx} = \\frac{dy}{dg}\\cdot\\frac{dg}{dx}\n",
    "\\end{align}\n",
    "\n",
    "在神经网络中,我们可以将每一层看作是一个子函数,整个网络就是这些子函数的复合。假设我们有一个两层神经网络,第一层的输出为 $h = g(\\vec{w}_1^T\\vec{x} + b_1)$,第二层的输出为 $y = f(\\vec{w}_2^Th + b_2)$,损失函数为 $L(y, t)$。我们想计算第一层权重 $w_{1i}$ 对损失函数的梯度:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_{1i}} &= \\frac{\\partial L}{\\partial y}\\cdot\\frac{\\partial y}{\\partial h}\\cdot\\frac{\\partial h}{\\partial w_{1i}} \\\\\n",
    "&= \\frac{\\partial L}{\\partial y}\\cdot f'(\\vec{w}_2^Th + b_2)\\cdot w_{2i}\\cdot g'(\\vec{w}_1^T\\vec{x} + b_1)\\cdot x_i\n",
    "\\end{align}\n",
    "\n",
    "通过链式法则,我们可以将梯度计算分解为多个子项,并逐层传播。这种端到端的误差传递机制使得深层网络可以被有效地训练。\n",
    "\n",
    "通过上述内容,我们介绍了训练神经网络的关键概念,包括误差函数、反向传播算法、偏导数计算和链式法则。这些概念为神经网络的优化提供了理论基础和实现方法,是深度学习领域的核心内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
