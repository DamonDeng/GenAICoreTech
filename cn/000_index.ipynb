{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI核心技术\n",
    "\n",
    "本文希望通过通俗易懂的方式给程序员出身的技术人们讲解GenAI核心技术。\n",
    "\n",
    "以下是内容目录：\n",
    "\n",
    "## 1. 向量和最简单的神经网络[链接](./001_vector_mlp.ipynb)\n",
    "\n",
    "### 1.1 什么是向量\n",
    "这个章节简单介绍了向量的概念，包括向量的数学表达方式和向量的维度概念，同时用图示表现了向量空间的概念。\n",
    "\n",
    "### 1.2 人造神经元\n",
    "人造神经元是构建神经网络的基本单元。它模仿生物神经元的工作原理,接收多个输入信号,对它们进行加权求和,然后通过一个激活函数产生输出信号。人造神经元的数学模型通常表示为y=f(Wx+b),其中x是输入向量,W是权重向量,b是偏置项,f是激活函数。\n",
    "\n",
    "### 1.3 多层神经网络 \n",
    "多层神经网络是由多个神经元层次构成的网络结构。输入层接收原始数据,中间隐藏层对数据进行特征提取和转换,输出层产生最终结果。每层神经元与上一层全部神经元相连,权重在训练过程中不断调整。多层结构赋予了神经网络强大的函数拟合能力。\n",
    "\n",
    "### 1.4 线性坍塌\n",
    "线性坍塌指的是在深层神经网络中,如果不使用合适的激活函数,那么多层线性变换的叠加实际上仍等价于一个单层线性变换,从而导致了\"网络深度无意义\"的问题,限制了网络的表达能力。引入非线性激活函数可以很好地缓解线性坍塌。\n",
    "\n",
    "### 1.5 激活函数\n",
    "激活函数为神经网络引入了非线性因素,是神经网络发挥强大功能的关键。常用的激活函数有Sigmoid、Tanh、ReLU等,它们对神经元输入的加权和进行非线性压缩或放大,使网络能够拟合非线性函数。不同的激活函数在不同的场景下有不同的优缺点和偏好。\n",
    "\n",
    "## 2. 训练一个神经网络[链接](./002_training_neuron_network.ipynb)\n",
    "\n",
    "### 2.1 误差\n",
    "训练神经网络的目标是最小化输出与期望值之间的误差。常用的误差函数有均方误差、交叉熵损失等,用于量化输出与标签之间的差异程度。通过优化算法如梯度下降,调整网络权重使得误差不断减小。\n",
    "\n",
    "### 2.2 反向传播\n",
    "反向传播是训练多层神经网络的核心算法,它沿着误差传播的方向,通过链式法则计算每个权重对误差的贡献,从而确定梯度方向,指导权重调整。这种端到端的误差传递机制使得深层网络可训练。\n",
    "\n",
    "### 2.3 偏导数\n",
    "偏导数是反向传播算法的基础,用于计算某个权重对误差的影响程度。通过对网络输出关于每个权重的偏导数,可以确定梯度的方向,从而指导权重调整。\n",
    "\n",
    "### 2.4 链式法则\n",
    "链式法则是计算复合函数导数的一种技巧,在反向传播中用于计算深层网络中的梯度。它将复合函数的导数分解为内部子函数导数的乘积,使得梯度计算可以遍历整个网络层次结构。\n",
    "\n",
    "## 3. 不同的网络结构 [链接](./003_network_architecture.ipynb)\n",
    "\n",
    "### 3.1 全连接神经网络(FCN)\n",
    "全连接神经网络中,每个神经元与上一层的所有神经元相连。这种密集连接方式使得网络具有很强的表达能力,可以学习任意的输入输出映射,但同时也导致了参数过多、计算量大的问题。FCN常用于传统的分类和回归任务。\n",
    "\n",
    "### 3.2 卷积神经网络(CNN)\n",
    "CNN通过卷积操作在局部区域内提取特征,大大减少了网络参数。同时池化操作赋予了一定的平移不变性。CNN在图像、语音等领域表现出色,成为深度学习的杀手级应用。常用的CNN结构有LeNet、AlexNet、VGGNet、ResNet等。\n",
    "\n",
    "### 3.3 循环神经网络(RNN)\n",
    "RNN擅长处理序列数据,通过内部状态的递归传递捕获序列的长期依赖关系。LSTM和GRU是改进的RNN变体,能更好地解决长期依赖问题。RNN广泛应用于自然语言处理、语音识别等领域。\n",
    "\n",
    "### 3.4 注意力机制(Attention)\n",
    "注意力机制赋予了神经网络专注于输入的不同部分的能力,突破了RNN的局限性。通过计算Query与Keys的相关性分数,对Values进行加权求和,实现对输入的软选择。注意力机制在机器翻译等任务中表现出色。\n",
    "\n",
    "### 3.5 Transformer\n",
    "Transformer完全基于注意力机制构建,摒弃了RNN的递归结构,大幅提升了并行计算能力。其Encoder-Decoder结构、位置编码等设计,使其在机器翻译等序列任务中表现卓越,成为当代最成功的语言模型之一。\n",
    "\n",
    "## 4. 深入了解Attention和Transformer [链接](./004_attention_and_transformer.ipynb)\n",
    "\n",
    "### 4.1 Query Key Value \n",
    "注意力机制的核心思想是通过Query查询与Key的相关性,对Value进行加权求和。其中Query、Key和Value可以是不同的表示,也可以是相同的表示。这种机制赋予了模型选择性地聚焦输入的能力。\n",
    "\n",
    "### 4.2 位置编码\n",
    "由于Transformer没有递归结构,因此需要一种方式来注入序列的位置信息。位置编码就是将序列的位置信息编码为向量,并与序列的表示相加,从而使得模型能够捕获元素在序列中的相对位置和顺序。\n",
    "\n",
    "### 4.3 Transformer编码器\n",
    "Transformer的编码器由多个相同的层组成,每层包含多头注意力子层和前馈神经网络子层。多头注意力子层对输入序列进行自注意力编码,前馈子层对每个位置的表示进行独立的非线性变换。\n",
    "\n",
    "### 4.4 Transformer解码器\n",
    "解码器与编码器类似,但增加了一个对编码器输出序列的注意力子层,以及遮挡未来位置的遮挡注意力机制,使得预测只依赖于当前及之前的输出。\n",
    "\n",
    "### 4.5 Encoder-Decoder架构\n",
    "Encoder-Decoder架构常用于序列到序列的任务,如机器翻译。编码器将输入序列编码为中间表示,解码器则基于该表示生成输出序列。两者之间通过注意力机制建立联系。\n",
    "\n",
    "### 4.6 Decoder-only架构\n",
    "Decoder-only架构常用于生成任务,如文本生成。模型的输入和输出都是文本序列,解码器需要对输入序列进行自回归建模,预测每个位置的标记仅依赖于之前的标记。\n",
    "\n",
    "## 5. LLM工作机制[链接](./005_LLM.ipynb)\n",
    "\n",
    "### 5.1 Prompt Engineering\n",
    "Prompt Engineering是指为LLM设计合适的提示语以获得理想的输出。良好的提示语可以为LLM提供必要的背景知识、任务描述和指导,从而引导模型生成所需的响应。\n",
    "\n",
    "### 5.2 从GPT到ChatGPT\n",
    "GPT是一种通用的预训练语言模型,通过自监督学习在大规模文本数据上预训练,获得广泛的知识。ChatGPT在GPT的基础上,进一步针对对话式交互进行了指令精调,增强了其理解和回应能力。\n",
    "\n",
    "### 5.3 max_token_size和其他参数\n",
    "LLM在生成响应时,输入和输出序列的长度通常受到max_token_size等参数的限制。其他常见参数包括温度(控制输出的随机程度)、Top-k/Top-p采样(控制输出的多样性)等,可用于调整生成质量。\n",
    "\n",
    "### 5.4 思维链\n",
    "思维链是LLM内部的一种推理机制,通过将复杂任务分解为一系列可操作的步骤,并逐步执行和跟踪这些步骤,从而解决问题。这种分步推理有助于LLM处理需要多步推理的复杂任务。\n",
    "\n",
    "### 5.5 函数调用\n",
    "LLM不仅可以生成自然语言,还可以被编程执行特定的函数,如数学计算、信息查询等。通过将函数调用的能力融入LLM,可以赋予它更强的功能扩展性和实用性。\n",
    "\n",
    "### 5.6 智能体(Agent)\n",
    "智能体是一种更高级的LLM系统,集成了多种功能如自然语言交互、任务规划、知识检索和函数调用等,可以自主地分析问题、制定计划并执行相应的操作,从而完成复杂的目标导向型任务。\n",
    "\n",
    "\n",
    "\n",
    "## 6. 扩散模型[链接](./006_diffusion_model.ipynb)\n",
    "\n",
    "### 6.1 扩散模型基本结构\n",
    "扩散模型由两个主要部分组成:一个正向扩散过程,将数据加入高斯噪声;一个反向过程,通过训练的神经网络从噪声中重建原始数据。这种结构使得模型能够从低分辨率开始,逐步生成高分辨率的图像或音频等数据。\n",
    "\n",
    "### 6.2 CLIP\n",
    "CLIP是一种用于图像-文本对比任务的模型,通过对大量图像-文本对进行对比式训练,学习将图像和文本映射到同一个潜在空间,从而实现跨模态的对比和检索。CLIP为扩散模型提供了强大的文本条件能力。\n",
    "\n",
    "### 6.3 U-Net\n",
    "U-Net是一种广泛应用于图像分割任务的卷积网络架构。它采用对称的编码器-解码器结构,使用跨层连接来融合不同分辨率的特征。U-Net常被用作扩散模型的主干网络,用于从噪声中重建图像。\n",
    "\n",
    "### 6.4 潜空间\n",
    "潜空间指的是模型学习到的低维连续表示空间,能够对原始高维数据(如图像)进行压缩和解压缩。扩散模型通过在潜空间中进行采样和推理,实现了对原始数据的高效生成。\n",
    "\n",
    "### 6.5 空间patch和时空patch\n",
    "在处理图像或视频等具有空间/时空结构的数据时,可以将其划分为多个patch(图像块或视频clip),并在patch级别上进行建模和生成,这有助于捕获局部结构信息。时空patch还能捕获时间上的动态信息。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
