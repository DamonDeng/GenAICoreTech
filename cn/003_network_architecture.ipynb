{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的,根据你提供的大纲,我将为第三节生成相应的内容。\n",
    "\n",
    "## 3. 不同的网络结构\n",
    "\n",
    "### 3.1 全连接神经网络 (Fully Connected Neural Network, FCN)\n",
    "\n",
    "全连接神经网络是最基本和最常见的神经网络结构。在这种网络中,每个神经元与上一层的所有神经元相连接。具体来说,如果上一层有 $n$ 个神经元,当前层有 $m$ 个神经元,那么就有 $n \\times m$ 个权重参数。\n",
    "\n",
    "全连接神经网络的数学表达式如下:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{y} = f(\\vec{W}^T\\vec{x} + \\vec{b})\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- $\\vec{x}$ 是输入向量,维度为 $n$\n",
    "- $\\vec{W}$ 是权重矩阵,维度为 $m \\times n$\n",
    "- $\\vec{b}$ 是偏置向量,维度为 $m$\n",
    "- $f$ 是激活函数,对矩阵进行元素级运算\n",
    "\n",
    "全连接神经网络的优点是具有很强的表达能力,可以学习任意的输入输出映射。然而,它也存在一些缺点:\n",
    "\n",
    "1. 参数过多,容易过拟合\n",
    "2. 计算量大,效率低下\n",
    "3. 缺乏平移不变性,对输入的空间位置敏感\n",
    "\n",
    "因此,全连接神经网络更适用于传统的分类和回归任务,如手写数字识别、图像分类等。但对于需要处理高维输入数据的任务,如图像处理、自然语言处理等,全连接神经网络可能会表现不佳。\n",
    "\n",
    "### 3.2 卷积神经网络 (Convolutional Neural Network, CNN)\n",
    "\n",
    "为了解决全连接神经网络的缺陷,卷积神经网络 (CNN) 被提出。CNN 通过卷积操作在局部区域内提取特征,大大减少了网络参数,同时池化操作赋予了一定的平移不变性。\n",
    "\n",
    "CNN 的基本结构包括卷积层、池化层和全连接层。卷积层通过滑动卷积核在输入数据上进行卷积操作,提取局部特征;池化层对卷积后的特征图进行下采样,减少数据量;全连接层则将提取到的特征映射到最终的输出。\n",
    "\n",
    "CNN 在图像、语音等领域表现出色,成为深度学习的杀手级应用。一些著名的 CNN 结构包括:\n",
    "\n",
    "- **LeNet**: 最早应用于手写数字识别的 CNN 结构。\n",
    "- **AlexNet**: 在 2012 年的 ImageNet 大赛中取得突破性成绩,引发了深度学习在计算机视觉领域的热潮。\n",
    "- **VGGNet**: 探索了更深的网络结构,证明了网络深度对性能的重要性。\n",
    "- **ResNet**: 通过引入残差连接解决了深层网络的梯度消失问题,在 ImageNet 大赛中取得了优异成绩。\n",
    "\n",
    "CNN 的发展促进了计算机视觉、图像处理等领域的快速进步,同时也推动了深度学习在其他领域的应用。\n",
    "\n",
    "### 3.3 循环神经网络 (Recurrent Neural Network, RNN)\n",
    "\n",
    "循环神经网络 (RNN) 是一种专门设计用于处理序列数据的神经网络结构。与前馈神经网络不同,RNN 通过内部状态的递归传递,能够捕获序列数据中的长期依赖关系。\n",
    "\n",
    "RNN 的基本思想是将序列数据一个个时间步地输入到网络中,每个时间步的输出不仅依赖于当前输入,还依赖于上一时间步的隐藏状态。数学表达式如下:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= f(W_{hx}x_t + W_{hh}h_{t-1} + b_h) \\\\\n",
    "y_t &= g(W_{yh}h_t + b_y)\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- $x_t$ 是时间步 $t$ 的输入\n",
    "- $h_t$ 是时间步 $t$ 的隐藏状态\n",
    "- $y_t$ 是时间步 $t$ 的输出\n",
    "- $W_{hx}$、$W_{hh}$、$W_{yh}$ 是权重矩阵\n",
    "- $b_h$、$b_y$ 是偏置向量\n",
    "- $f$、$g$ 是激活函数\n",
    "\n",
    "虽然 RNN 理论上可以捕获任意长度的序列依赖关系,但在实践中,它们往往存在梯度消失或梯度爆炸的问题,导致无法有效地学习长期依赖。为了解决这个问题,研究者提出了一些改进的 RNN 变体,如长短期记忆网络 (Long Short-Term Memory, LSTM) 和门控循环单元 (Gated Recurrent Unit, GRU)。\n",
    "\n",
    "RNN 及其变体广泛应用于自然语言处理、语音识别等领域,成为处理序列数据的主要工具。\n",
    "\n",
    "### 3.3.1 长短期记忆网络(Long Short-Term Memory, LSTM)\n",
    "\n",
    "LSTM是一种特殊的RNN,旨在解决传统RNN存在的梯度消失/爆炸问题,使其能够更好地捕获长期依赖关系。LSTM的核心思想是引入门控机制,通过特殊设计的门结构来控制状态的更新和传递。\n",
    "\n",
    "LSTM的结构包括一个细胞状态向量,以及控制该状态的三个门:遗忘门、输入门和输出门。数学表达式如下:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_t &= \\sigma(W_f[h_{t-1}, x_t] + b_f) &\\text{(遗忘门)} \\\\\n",
    "i_t &= \\sigma(W_i[h_{t-1}, x_t] + b_i) &\\text{(输入门)}\\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C[h_{t-1}, x_t] + b_C) &\\text{(候选细胞状态)} \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t &\\text{(细胞状态)} \\\\\n",
    "o_t &= \\sigma(W_o[h_{t-1}, x_t] + b_o) &\\text{(输出门)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) &\\text{(隐藏状态)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "其中:\n",
    "- $f_t$、$i_t$、$o_t$ 分别是遗忘门、输入门和输出门的激活值向量\n",
    "- $C_t$ 是当前时间步的细胞状态向量\n",
    "- $\\tilde{C}_t$ 是当前时间步的候选细胞状态向量\n",
    "- $\\sigma$ 是sigmoid激活函数,用于门值的计算\n",
    "- $\\odot$ 表示元素级乘积运算\n",
    "\n",
    "LSTM通过精细设计的门控机制,使得细胞状态可以很好地控制信息的保留和遗忘,从而缓解了梯度消失/爆炸问题。LSTM在自然语言处理、语音识别等序列建模任务中表现出色。\n",
    "\n",
    "### 3.4 残差网络(Residual Network, ResNet)\n",
    "\n",
    "随着神经网络层数的增加,会出现\"网络退化\"(degradation)问题,即准确率会出现饱和,甚至下降。为了解决这一问题,残差网络(ResNet)被提出。\n",
    "\n",
    "ResNet的核心思想是在神经网络中引入\"残差连接\"(residual connection),将输入直接传递到后面的层,并与该层的输出相加。这样做的目的是让网络只需要学习输入和输出之间的\"残差\"部分,而不是整个映射关系,从而简化了学习目标。\n",
    "\n",
    "具体来说,如果将普通层的输出记为$H(x)$,输入为$x$,那么ResNet的输出为:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y = H(x) + x\n",
    "\\end{aligned}$$\n",
    "\n",
    "其中,+表示元素级相加操作。\n",
    "\n",
    "ResNet的残差结构不仅能够有效缓解梯度消失/爆炸问题,还使得网络可以变得更深,从而获得更强的表达能力。自提出以来,ResNet在计算机视觉领域取得了卓越的成绩,成为了CNN的重要变体。\n",
    "\n",
    "值得一提的是,除了ResNet,还有一些其他的残差网络结构,如高级ResNet、DenseNet等,它们在ResNet的基础上进行了改进和扩展。\n",
    "\n",
    "### 3.5 注意力机制 (Attention Mechanism)\n",
    "\n",
    "注意力机制赋予了神经网络专注于输入的不同部分的能力,突破了 RNN 在处理长期依赖方面的局限性。注意力机制的基本思想是,在生成每个输出时,根据当前的输入和状态,计算出一个注意力分数向量,表示对输入的不同部分的关注程度。然后,将输入加权求和,得到注意力加权的表示,用于生成最终输出。\n",
    "\n",
    "注意力机制的数学表达式如下:\n",
    "\n",
    "\\begin{align}\n",
    "a_t &= \\text{Attention}(q_t, K) \\\\\n",
    "v_t &= \\sum_{i=1}^{n} a_{ti}V_i \\\\\n",
    "y_t &= f(v_t, q_t)\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "- $q_t$ 是查询向量 (Query)\n",
    "- $K$ 是键值对 (Key-Value Pairs)\n",
    "- $a_t$ 是注意力分数向量\n",
    "- $V_i$ 是值向量 (Value)\n",
    "- $v_t$ 是注意力加权的表示\n",
    "- $y_t$ 是输出\n",
    "\n",
    "注意力机制在机器翻译等任务中表现出色,成为了 RNN 之外的另一种处理序列数据的重要方法。\n",
    "\n",
    "### 3.6 Transformer\n",
    "\n",
    "Transformer 是一种完全基于注意力机制构建的神经网络架构,它摒弃了 RNN 的递归结构,大幅提升了并行计算能力。Transformer 的核心组件包括编码器 (Encoder) 和解码器 (Decoder)。\n",
    "\n",
    "**编码器**由多个相同的层组成,每层包含多头注意力子层和前馈神经网络子层。多头注意力子层对输入序列进行自注意力编码,前馈子层对每个位置的表示进行独立的非线性变换。\n",
    "\n",
    "**解码器**与编码器类似,但增加了一个对编码器输出序列的注意力子层,以及遮挡未来位置的遮挡注意力机制,使得预测只依赖于当前及之前的输出。\n",
    "\n",
    "Transformer 还引入了位置编码的概念,将序列的位置信息编码为向量,并与序列的表示相加,从而使得模型能够捕获元素在序列中的相对位置和顺序。\n",
    "\n",
    "Transformer 的 Encoder-Decoder 架构常用于序列到序列的任务,如机器翻译。编码器将输入序列编码为中间表示,解码器则基于该表示生成输出序列。两者之间通过注意力机制建立联系。\n",
    "\n",
    "Transformer 在机器翻译等序列任务中表现卓越,成为当代最成功的语言模型之一。它的出现也推动了注意力机制在自然语言处理领域的广泛应用。\n",
    "\n",
    "通过上述介绍,我们了解了几种常见的神经网络结构,包括全连接神经网络、卷积神经网络、循环神经网络、注意力机制和 Transformer。不同的网络结构适用于不同的任务和数据类型,深度学习的发展离不开对这些结构的探索和创新。\n",
    "\n",
    "\n",
    "好的,我来补充LSTM和残差网络的部分:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
